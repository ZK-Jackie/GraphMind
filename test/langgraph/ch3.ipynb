{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangGraph ä½“éªŒ\n",
    "\n",
    "## Chapter 3: LangGraph with Human\n",
    "\n",
    "Human in the loop æ˜¯æŒ‡åœ¨ Agent çš„å·¥ä½œæµä¸­ï¼Œæ’å…¥äººç±»çš„å‚ä¸ã€‚è¿™æ ·çš„è®¾è®¡å°†ä¾¿äºæ›´å¥½åœ°æ§åˆ¶å·¥ä½œæµçš„èµ°å‘ï¼Œè¿ç€ç”¨æˆ·æ›´åŠ æ»¡æ„çš„æ–¹å‘ã€‚\n",
    "\n",
    "å…¶å®å‰æ–‡æ”¹å˜ã€æ›´æ–°ã€æ“çºµå·¥ä½œæµçš„è¿‡ç¨‹å·²ç»å……åˆ†ä½“ç°äº† Human in the loop çš„è®¾è®¡æ€æƒ³ï¼Œéƒ½æ˜¯å€ŸåŠ©äº†å¤–éƒ¨çš„å› ç´ æ¥æ§åˆ¶å·¥ä½œæµçš„èµ°å‘ã€‚ä½†ä¸ºäº†æ›´åŠ æ–¹ä¾¿çš„å®ç°ç”¨æˆ·åœ¨ Agent å†³ç­–è¿‡ç¨‹ä¸­çš„ä½œç”¨ï¼Œæˆ‘ä»¬è¿˜èƒ½å¤Ÿè®©äººç±»èƒ½å¤Ÿæ›´åŠ æœ‰é€‰æ‹©æ€§åœ°å‚ä¸åˆ°å·¥ä½œæµä¸­ã€‚\n",
    "\n",
    "æ­£å¦‚æˆ‘ä»¬ä¸€ç›´æ‰€è¯´çš„ï¼ŒState ç±»æ˜¯æ•´ä¸ªå·¥ä½œæµä¸­çš„å…³é”®å˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®šåˆ¶åŒ–çš„ State ç±»æ¥æ›´å¥½å®ç° Human in the loop çš„è®¾è®¡ã€‚"
   ],
   "id": "8528e4b7e3baeac7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "LLM_NAME = os.getenv(\"ZHIPU_LLM_NAME\") or None\n",
    "API_BASE = os.getenv(\"ZHIPU_API_BASE\") or None\n",
    "API_KEY = os.getenv(\"ZHIPU_API_KEY\") or None"
   ],
   "id": "c9cd4bb174ca12f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. è‡ªå®šä¹‰ State ç±» â€”â€” State ä¸ä¸ªå·¥ä½œæµèŠ‚ç‚¹çš„æœ‰æœºç»“åˆ\n",
    "\n",
    "State ç±»å¯¹äºæ•´ä¸ªæœ‰å‘å›¾çš„æ„å»ºæ˜¯è‡³å…³é‡è¦çš„ï¼Œä¸‹æ–‡æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå«äººçš„è°ƒç”¨çŠ¶æ€çš„ State ç±»ï¼Œä»¥ä¾¿æ›´å¥½åœ°å®ç° Human in the loop çš„è®¾è®¡ã€‚\n",
    "\n",
    "å‡å®šå½“å‰æœ‰ä¸€ä¸ªä¸šåŠ¡ï¼Œç”¨æˆ·é—®äº†ä¸€ä¸ªéå¸¸éå¸¸ä¸“ä¸šçš„é—®é¢˜ï¼Œä½†æ˜¯ LLM å‘ç°è‡ªå·±ä¸å¤ªèƒ½å›ç­”ï¼Œåˆ™æ­¤æ—¶éœ€è¦å¦ä¸€ä¸ªä¸“å®¶æ¥å¸®å¿™å›ç­”ã€‚åˆ™æ­¤æ—¶è¢«æ‰¾çš„ä¸“å®¶å³ä¸ºæˆ‘ä»¬è¦å®ç°çš„ Human in the loop ä¸­çš„äººç±»ï¼"
   ],
   "id": "c2ef77c3b9e2b65d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # This flag is new\n",
    "    ask_human: bool"
   ],
   "id": "fd0fb22ea2dbb902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "éšåï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª Pydantic ç±»ï¼Œç”¨äºå®šä¹‰ä¸€ä¸ªè¯·æ±‚ä¸“å®¶å¸®åŠ©çš„è¯·æ±‚ã€‚\n",
    "\n",
    "å®˜æ–¹æ–‡æ¡£æç¤ºæˆ‘ä»¬è¦ç”¨ pydantic v2 è€Œä¸æ˜¯ v1"
   ],
   "id": "e0c01621d62c4481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class RequestAssistance(BaseModel):\n",
    "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    request: str"
   ],
   "id": "fa0fa6fae79f3935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å†æ¬¡å®šä¹‰ä¸€ä¸ª chatbot èŠ‚ç‚¹ï¼Œä½†è¿™æ¬¡åœ¨ä¸¤å¤„æœ‰æ”¹åŠ¨ã€‚\n",
    "\n",
    "- ä¸€æ˜¯å¤šç»‘å®šäº†ä¸€ä¸ªå·¥å…·ï¼ŒRequestAssistanceï¼Œç”¨äºæ¥å—ç”¨æˆ·çš„è¯·æ±‚å¹¶å°†å…¶è½¬å‘ç»™ä¸“å®¶ã€‚åœ¨ LangChain ä¸­ï¼Œå·¥å…·çš„å®šä¹‰å¯ä»¥æ˜¯ä¸€ä¸ª Pydantic ç±»ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªç»§æ‰¿äº† BaseTool çš„ç±»ã€‚\n",
    "- äºŒæ˜¯åœ¨ chatbot å‡½æ•°ä¸­ï¼Œå½“ LLM è®¤ä¸ºè‡ªå·±ä¸èƒ½å›ç­”çš„æ—¶å€™ï¼Œä¼šå°† ask_human è®¾ä¸º Trueï¼Œæ›´æ–° stateï¼Œä»¥ä¾¿åç»­çš„å·¥ä½œæµèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è¿™ç§æƒ…å†µã€‚"
   ],
   "id": "e5a2aae2b9c2ec3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=5)\n",
    "tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI(model=LLM_NAME, openai_api_key=API_KEY, openai_api_base=API_BASE)\n",
    "llm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n",
    "\n",
    "def chatbot(state: State):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    ask_human = False\n",
    "    if (\n",
    "        response.tool_calls\n",
    "        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
    "    ):\n",
    "        ask_human = True\n",
    "    return {\"messages\": [response], \"ask_human\": ask_human}"
   ],
   "id": "6483803b045e9619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "éšåå°±æ˜¯å¾ˆæ™®é€šçš„æ„å»ºå·¥å…·ã€æ„å»ºå›¾çš„è¿‡ç¨‹äº†ã€‚",
   "id": "13ede349cb911850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=[tool]))"
   ],
   "id": "76eee15c8eda4ee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "# æ­¤å¤„å®šä¹‰äº†ä¸€ä¸ªå·¥å…·å‡½æ•°ï¼Œç”¨äºæ ¹æ® AI æ¶ˆæ¯çš„å·¥å…·è¯·æ±‚æ¶ˆæ¯ï¼Œæ„å»ºä¸€ä¸ªå·¥å…·å›ç­”æ¶ˆæ¯\n",
    "def create_response(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response,\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    new_messages = []\n",
    "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        # Typically, the user will have updated the state during the interrupt.\n",
    "        # If they choose not to, we will include a placeholder ToolMessage to\n",
    "        # let the LLM continue.\n",
    "        new_messages.append(\n",
    "            create_response(\"No response from human.\", state[\"messages\"][-1])\n",
    "        )\n",
    "    return {\n",
    "        # Append the new messages\n",
    "        \"messages\": new_messages,\n",
    "        # Unset the flag\n",
    "        \"ask_human\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"human\", human_node)"
   ],
   "id": "2fe048e3821a32c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å‰æ–‡ä¸€ç›´ååå¤å¤çš„è°ƒæ•´åè½¬ state å¯¹è±¡ä¸­ `ask_human` çš„å€¼ï¼Œç›®çš„å°±æ˜¯ä¸ºäº†æ§åˆ¶ä»–æ˜¯å¦èƒ½å¤Ÿé€‰æ‹© human å·¥å…·èŠ‚ç‚¹ã€‚æœ‰ä¸€ç§æ€æƒ³æ˜¯åœ¨å·¥å…·å†…éƒ¨ï¼ŒæŸ¥çœ‹ `state.ask_human` çš„å€¼ï¼Œä½†æ˜¯æ›´ä¸ºç²¾å‡†çš„åšæ³•åˆ™æ˜¯ä½¿ç”¨æ¡ä»¶è¾¹ã€‚\n",
    "\n",
    "æ¡ä»¶è¾¹çš„ç”¨æ³•å¯è§ ch1ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†ä¸Šæ–‡å®šä¹‰çš„å†…å®¹éƒ½ç»„è£…æˆä¸€ä¸ªå®Œæ•´çš„æœ‰å‘å›¾ï¼Œç„¶åä½¿ç”¨ MemorySaver æ¥ä¿å­˜ä¸­é—´çŠ¶æ€ã€‚å¦‚ä¸‹æ–‡ã€‚"
   ],
   "id": "c9122ef31fed5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "def select_next_node(state: State):\n",
    "    if state[\"ask_human\"]:\n",
    "        return \"human\"\n",
    "    # Otherwise, we can route as before\n",
    "    return tools_condition(state)\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    select_next_node,\n",
    "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "\n",
    "# The rest is the same\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"human\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # We interrupt before 'human' here instead.\n",
    "    interrupt_before=[\"human\"],\n",
    ")"
   ],
   "id": "bc238490bd59f0d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "è®©æˆ‘ä»¬å°è¯•é—®ä¸€ä¸ªä¸“ä¸šé—®é¢˜ï¼Œå¼•å¯¼ LLM é€‰å– human èŠ‚ç‚¹ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆå§ï¼",
   "id": "69ad6df81b406b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_input = \"I need some expert guidance for building this AI agent. Could you request assistance for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "b024f9c1119f6959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ],
   "id": "95b7f8972131ae63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "é€šè¿‡æŸ¥çœ‹ç”¨æˆ·é—®è¯¢ç»“æŸåçš„ graph çŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯ human èŠ‚ç‚¹ã€‚\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬å°±è¿™æ ·æ¨è¿›ä¸‹å»ï¼Œgraph å°±ä¼šè¿›å…¥ human èŠ‚ç‚¹äº†ï¼Œä½†æ˜¯è¿™ä¸ªç¨‹åºåœ¨è®¾è®¡ human èŠ‚ç‚¹æ—¶ï¼Œé»˜è®¤ç”¨æˆ·åœ¨ human èŠ‚ç‚¹ä¸­è·å–ä¸åˆ°ä»»ä½•ä¿¡æ¯ï¼Œåˆ™æˆ‘ä»¬æ­¤æ—¶å¯ä»¥é€šè¿‡äººä¸ºæ·»åŠ å¯¹è¯æ¥æ¨¡æ‹Ÿ human çš„å›ç­”ã€‚"
   ],
   "id": "d8b24cc8d72aafe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ai_message = snapshot.values[\"messages\"][-1]\n",
    "human_response = (\n",
    "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
    "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
    ")\n",
    "tool_message = create_response(human_response, ai_message)\n",
    "graph.update_state(config, {\"messages\": [tool_message]})\n",
    "for message_pieces in graph.get_state(config).values[\"messages\"]:\n",
    "    message_pieces.pretty_print()"
   ],
   "id": "15b08ab222c4c041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "åœ¨ä¸Šé¢çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆä¼ªé€ äº†ä¸€ä¸ªæ¥è‡ª human çš„å›ç­”ï¼Œæœ€åå†äº¤ç»™ LLM çœ‹çœ‹ä»–ä¼šæ€ä¹ˆå¤„ç†ï¼Œæœ€åæˆ‘ä»¬å†å›é¡¾ä¸€ä¸‹è¿™æ¬¡å¯¹è¯çš„å…¨éƒ¨å†…å®¹å§ï¼\n",
    "\n",
    "å¤ä¹ ä¸€ä¸‹ï¼Œå†æ¬¡è°ƒç”¨ stream() æ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ç»™ Noneï¼Œå°†ä¼šä»æ–­ç‚¹å¤„ç»§ç»­æ¨è¿› graph çš„çŠ¶æ€ã€‚"
   ],
   "id": "21d34db427db69dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for message_pieces in graph.get_state(config).values[\"messages\"]:\n",
    "    message_pieces.pretty_print()"
   ],
   "id": "7df46b6d8ddfa6e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. å€’å¸¦ ï¼ˆåšä¸å‡ºğŸ˜­ğŸ˜­ğŸ˜­ï¼‰\n",
    "\n",
    "è‹¥åœ¨æŸäº›å¯¹è¯è¿‡ç¨‹å½“ä¸­ï¼Œç”¨æˆ·æƒ³è¦ä¿®æ”¹å…¶ä¸­æŸå¥è¯çš„è¡¨è¿°ï¼Œæƒ³è¦å›åˆ°æŸä¸ªç‰¹å®šçš„ LLM ä¸Šä¸‹æ–‡çŠ¶æ€æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "è¿™ç§æƒ…å†µä¸‹å°±è¦ç”¨åˆ° LangGraph ä¸­ graph å¯¹è±¡çš„ `get_state_history` äº†ï¼\n",
    "\n",
    "æ‰¿æ¥ä¸Šæ–‡ï¼Œè®©æˆ‘ä»¬æ¥æ„é€ ä¸€æ¬¡æ™®é€šçš„é—®ç­”è¯·æ±‚ã€‚ä¸ºäº†é¿å…å‰æ–‡æ¶ˆæ¯çš„å¹²æ‰°ï¼Œæˆ‘ä»¬é‡æ–°æ„é€ ä¸€ä¸ªä¸¤è½®å¯¹è¯ï¼Œä»¤ `thread_id` ä¸º 3ã€‚"
   ],
   "id": "4830d625801b8221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"I'm learning LangGraph. Could you do some web research on it for me?\")\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "4b2875c4ce76ce7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ç°åœ¨å†å†™ä¸€ä¸ªä»£ç ï¼Œæ¥æŸ¥çœ‹ graph çš„çŠ¶æ€å†å²å§ã€‚",
   "id": "1d15d174d669dc35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_replay = None\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    if len(state.values[\"messages\"]) == 6:\n",
    "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay = state   # è¿™ä¸ª to_replay å®šä¸º 6ï¼Œå³ç¬¬äºŒè½®å¯¹è¯ç»“æŸåçš„çŠ¶æ€â€”â€”æœ€æ–°çŠ¶æ€ï¼Œæœ‰ä»€ä¹ˆç”¨ï¼Ÿï¼Ÿ"
   ],
   "id": "d3b65cffa31c826b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "é€šè¿‡ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬è·å–åˆ°äº†ä¸€ä¸ªç›®æ ‡çš„çŠ¶æ€ç‚¹ï¼Œæ‰“å°ä¸€ä¸‹çœ‹çœ‹ï¼Ÿ",
   "id": "cf30829957c88a59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(to_replay.next)\n",
    "print(to_replay.config)"
   ],
   "id": "f2205d5baa4573b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬åˆšæ‹¿åˆ°çš„è¿™ä¸ªçŠ¶æ€ç‚¹ä¸‹ä¸€æ­¥æ²¡äº‹å¹²ï¼ˆ`to_replay.next` å†…å®¹ä¸ºç©ºï¼‰ï¼Œè€Œä¸” config ä¹Ÿæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚ä½†è¿™å¹¶ä¸é‡è¦ï¼Œé‡ç‚¹æ˜¯æˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™æ ·ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œé‡æ–°æ¨è¿›å¯¹è¯ã€‚",
   "id": "6a70cf77a4e24627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
    "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "393c0c7814f3dc29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "éš¾é“è¿™ä¸ªæ£€æŸ¥ç‚¹çš„æ„ä¹‰å°±å±€é™äºæ­¤ï¼Ÿå°±è¿™ä¹ˆæ— èŠä¸€ä¸ªåº”ç”¨ï¼Ÿ\n",
    "\n",
    "ä¸‹æ–‡å°†ä»‹ç»å¦‚ä½•æ›´æ–°å…¶ä¸­çš„æŸä¸€ä¸ªæ­¥éª¤ï¼Œå¹¶ä¸”é‡æ–°ä»é‚£ä¸€ä¸ªæ­¥éª¤å¼€å§‹æ¨è¿›å¯¹è¯ã€‚\n",
    "\n",
    "ï¼ˆå¥½çƒ¦ï¼Œè¿™ä¸ªå€’å¸¦åšä¸å‡ºï¼Œä¸çŸ¥é“å“ªé‡Œå‡ºé—®é¢˜äº†ğŸ˜­ğŸ˜­ğŸ˜­ï¼‰"
   ],
   "id": "b62c07a0f167f9e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_replay_again = None\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    # è¿™æ ·å†™ if æ˜¯å› ä¸ºåˆ° START èŠ‚ç‚¹è·Ÿåˆ°ä¸Šä¸€è½®çš„ END èŠ‚ç‚¹çš„ messages é•¿åº¦æ˜¯ä¸€æ ·çš„ï¼ˆéƒ½æ˜¯4ï¼Œå¯ä»¥è§‚å¯Ÿä¹‹å‰çš„ state é™ˆåˆ—ç»“æœï¼‰ã€‚è¦å€’å¸¦è‚¯å®šæ˜¯å€’åˆ°æœ‰å®é™… message çš„åœ°æ–¹ï¼ˆå³ START å¤„ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œçš„ if è¯­å¥æ˜¯ä¸ºäº†æ‰¾åˆ° START èŠ‚ç‚¹ï¼Œæ€•è·Ÿä¸Šä¸€è½®çš„ END èŠ‚ç‚¹æ··æ·†ã€‚\n",
    "    # æ€»ç»“ï¼šä¸ºäº†æ‰¾åˆ° START èŠ‚ç‚¹ï¼Œè§„é¿æ‰ä¸Šä¸€è½®çš„ END èŠ‚ç‚¹ã€‚\n",
    "    if len(state.values[\"messages\"]) == 4:  \n",
    "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay_again = state\n",
    "\n",
    "print(\"\\n\\n replay node: \")\n",
    "print(to_replay_again.metadata)\n",
    "print(to_replay_again.next)\n",
    "print(to_replay_again.config)\n",
    "\n",
    "for event in graph.stream(None, to_replay_again.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "4241f1c0d19c2530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "last_message = to_replay_again.values[\"messages\"][-1]\n",
    "\n",
    "# ä¿®æ”¹ç¬¬äºŒè½®å¯¹è¯çš„ç”¨æˆ·è¾“å…¥\n",
    "last_message.content = \"Thanks you! I will take your information and build a chatbot with LangGraph! Before that, I also wanted to know about LangChain, could you help me with that?\"\n",
    "\n",
    "branch_config = graph.update_state(\n",
    "    to_replay_again.config,\n",
    "    {\"messages\": [last_message]},\n",
    ")\n",
    "print(branch_config)"
   ],
   "id": "e9e46c1036962201",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.stream(None, branch_config, stream_mode=\"values\")",
   "id": "ff1fb220dd22447c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for message_pieces in graph.get_state(config).values[\"messages\"]:\n",
    "    message_pieces.pretty_print()"
   ],
   "id": "c558cb35eda60267",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
